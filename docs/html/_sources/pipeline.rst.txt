=============================
Complete Pipeline Description
=============================

This section of the manual constitutes a reasonably complete description of what each task of the pipeline actually
does. Where practical, I have attempted to provide in some closed mathematical form an example of the actual resultant
data product. Where not practical, I have attempted enough verbal description of the output to, hopefully, explain,
in conjunction with the code and the comments within it.

As mentioned elsewhere, the pipeline employs the ``yenta`` task runner to coordinate its operations. The job of the
task runner is to resolve the dependencies among tasks and ensure that the proper parameters are being fed from
upstream tasks to downstream ones. The image below shows the task graph with dependencies at the top feeding
tasks below them.

.. image:: images/hb_task_graph.png

You can also download :download:`the task graph <images/hb_task_graph.pdf>` to zoom in.

--------
Notation
--------

The following notation is used throughout this document:

.. table::
    :widths: auto

    ======================             ===========
    Notation                           Explanation
    ======================             ===========
    :math:`\mathbf{M}`                 a matrix
    :math:`g_{i}`                      the i\ :sup:`th` gene
    :math:`D_{i}`                      the i\ :sup:`th` dataset
    :math:`\mathbf{D}_{i}`             the matrix representing the i\ :sup:`th` dataset
    :math:`C_{i}`                      the i\ :sup:`th` context
    :math:`\mathbf{C}_{i}`             the matrix representing the i\ :sup:`th` context
    :math:`T_{i}`                      the i\ :sup:`th` Gene Ontology term
    ``some_function``                  a named task within the pipeline
    ======================             ===========

------------------------
Data Sources and Mapping
------------------------

Data sources used in HumanBase come in two types: primary and auxiliary. Primary data sources are used directly within
the inference process, while auxiliary data sources are used to aid transformations on primary data sources
(e.g. to map gene identifiers). The same data repository may provide both auxiliary and primary data sources;
typically, auxiliary data sources must be processed before primary ones, since the auxiliary sources contain information
that allows primary sources to be transformed properly.

The data sources generated by any task will be summarized in a table at the end of the task description section.
I have attempted to stick to nomenclature which matches the names of the variables used in the functions but this is
not always convenient.

The end product of every series of transformations on a primary data source is a matrix representing a gene connectivity
network, with the value of any entry in the matrix representing the edge weight between the two genes in the network as
quantized into one of 8 bins. That is, for any dataset :math:`D`, the final form of the resultant matrix is
:math:`\mathbf{D}_{ij} \in \{0, \ldots, 7\}`. The digitization bins are specified in the respective sections.

.. _set_up_directories:

------------------
set_up_directories
------------------

The root task, which simply creates a bunch of directories in which the expected output of the other tasks will be
placed. These directories are created locally and at some point this task should probably be changed to properly
symlink to ``ceph`` on the Flatiron cluster. The names of the directories are legacies from a previous dependency
on the Sleipnir library and correspond to the names of its various CLI tools.

.. warning::
    You should almost certainly never re-run this task. Since it is the root task, it will trigger all downstream
    tasks to run again, which is probably not what you want. Future developers may consider eliminating this task
    entirely and putting its logic into some one-time setup command.

.. _biocyc_etl:

-----------------
biocyc_etl
-----------------

This task is responsible for downloading and preprocessing data from the
`BioCyc repository <https://brg.ai.sri.com>`_ which is then used to construct the gold standard. BioCyc
contains the following data sources:

.. table::
    :widths: auto

    =============     =========  =======
    Data Source       Type       Purpose
    =============     =========  =======
    GeneMapper        Auxiliary  Provides a mapping between UniProt IDs and MetaCyc IDs
    Genes             Auxiliary  Provides a list of genes indexed by a MetaCyc ID
    Pathways          Auxiliary  Provides a list of pathways
    PathwaysGenes     Primary    A mapping between genes and the pathways they participate in
    =============     =========  =======

The auxiliary mapping data is written to a database table. In preprocessing, the *PathwaysGenes* data is aggregated by
pathway with array aggregation, such that the result is a table with one column being the pathway ID and the second
column being the list of all the genes that participate in that pathway. Any two genes that share the same pathway
are marked as connected to each other with an edge weight of 1.

.. _interactor_etl::

-------------------------------------------
biogrid_etl, intact_etl, mint_etl, mips_etl
-------------------------------------------

These tasks are responsible for the download and preprocessing of data from the
`BioGrid <https://thebiogrid.org>`_, `EMBL <https://www.ebi.ac.uk/>`_, and
`IBIS <https://www.helmholtz-munich.de/>`_ repositories.
BioGrid, Intact, MIPS, and MINT are all "interactor" data sources. This means that the raw data from each of these
repositories indicates, for every gene pair, which interactions connect the genes. There can be more than one row for
any given gene pair. All four datasets are transformed in the same way, so their respective ETL tasks have been
condensed into one section.

.. table::
    :widths: auto

    =============     =========  =======
    Data Source       Type       Purpose
    =============     =========  =======
    Interactions      Primary    Provides a list of gene pairs and the interactions between them
    =============     =========  =======

The *Interactions* data is aggregated over the gene pairs such that the edge weight between any two genes is the count
of the number of distinct interaction methods that are used to confirm interactions between the two genes. The resulting
matrix is then digitized according to the bins above.

.. _ncbi_etl:

--------
ncbi_etl
--------

This task is responsible for downloading data from the `NCBI <https://www.ncbi.nlm.nih.gov/>`_ repository. It is a
unique task in that all of its data products are auxiliary and generate mappings which are then used to translate
identifiers from various other data sources into Entrez identifiers.

.. table::
    :widths: auto

    ====================== =========  =======
    Data Source            Type       Purpose
    ====================== =========  =======
    GeneInfoMIMMapper      Auxiliary  Provides a list of mappings between MIM IDs and Entrez IDs
    GeneInfoHugoMapper     Auxiliary  Provides a list of mappings between Hugo IDs and Entrez IDs
    GeneInfoEnsemblMapper  Auxiliary  Provides a list of mappings between Ensembl IDs and Entrez IDs
    GeneInfoIMGTMapper     Auxiliary  Provides a list of mappings between IMGT IDs and Entrez IDs
    GeneInfoMirMapper      Auxiliary  Provides a list of mappings between Mir IDs and Entrez IDs
    GeneInfoSGDMapper      Auxiliary  Provides a list of mappings between SGD IDs and Entrez IDs
    GeneInfoFlybaseMapper  Auxiliary  Provides a list of mappings between Flybase IDs and Entrez IDs
    GeneInfoWormbaseMapper Auxiliary  Provides a list of mappings between Wormbase IDs and Entrez IDs
    GeneInfo               Auxiliary  Provides a list of Entrez gene IDs by taxon
    ====================== =========  =======

.. _gsea_etl:

--------
gsea_etl
--------

This task is responsible for downloading and preprocessing data from the `GSEA <https://www.gsea-msigdb.org/gsea/msigdb>`_
repository.

.. table::
    :widths: auto

    ==============     =========  =======
    Data Source        Type       Purpose
    ==============     =========  =======
    GSEAMotif          Primary    Provides a mapping between motifs and the genes that are associated through them
    GSEAExperiment     Primary    Provides a mapping between experiments and the genes that are associated through them
    ==============     =========  =======

.. _gene_ontology_etl:

-----------------
gene_ontology_etl
-----------------

This task is responsible for dcownloading and preprocessing the data from the `Gene Ontology <http://geneontology.org/>`_
repository. The gene ontology is at the core of the operations of the pipeline as it is the thing used to derive the
contexts and the genes that participate in them. Unlike other transformers, the gene ontology transformer turns the
text file of the ontology into a `NetworkX <https://networkx.org/>`_ network that can be navigated with the standard
graph algorithms. This task also generates an annotation file which allows us to

.. table::
    :widths: auto

    ========================     =========  =======
    Data Source                  Type       Purpose
    ========================     =========  =======
    GeneOntologyNetwork          Primary    Provides a NetworkX network that represents the connectivity between ontology terms
    GeneOntologyAnnontations     Auxiliary  Provides annotations for which allows ontology terms to be annotated with proteins
    ========================     =========  =======

.. _refine_bio_etl:

--------------
refine_bio_etl
--------------

This task downloads and preprocesses data from the `RefineBio <https://refine.bio>`_ repository. This repository represents
the bulk of the data that is actually used in the inference process, and consists of microarray expression data with
each row represents a gene and each column represents the expression level in a given sample. The actual files
downloaded from RefineBio are individual sample files that are then stitched into the PCL files that have the above
structure, with one file for each experiment.

.. table::
    :widths: auto

    ===================    =========  =======
    Data Source            Type       Purpose
    ===================    =========  =======
    RefineBioPCL           Primary    Provides a list of PCL files representing the expression level of genes in microarray experiments.
    ===================    =========  =======

.. _refine_bio_compendia_etl:

------------------------
refine_bio_compendia_etl
------------------------

This task downloads SeqRNA data from RefineBio and stitches the resultant Salmon files into aggregate PCL files that,
as above, contain genes along the rows and samples along the columns. This data is not actually used in the inference
process, but theoretically it should be straightforward to integrate it into the pipeline.

.. table::
    :widths: auto

    ===================    =========  =======
    Data Source            Type       Purpose
    ===================    =========  =======
    RefineBioPCL           Primary    Provides a list of PCL files representing the expression level of genes in microarray experiments.
    ===================    =========  =======

.. _uniprot_etl:

-----------
uniprot_etl
-----------

This task downloads and preprocesses data from the `UniProt <https://www.uniprot.org/>`_ to provide mappings between
proteins and the genes responsible for coding for them. This auxiliary data is used to map protein annotations on the
gene ontology terms to actual genes.

.. table::
    :widths: auto

    ===================    =========  =======
    Data Source            Type       Purpose
    ===================    =========  =======
    UniProtMap             Auxiliary  Provides mappings between proteins and the genes that code for them
    ===================    =========  =======

.. _ensembl_etl:

-----------
ensembl_etl
-----------

This task downloads and preprocesses Ensembl data to provide a mapping between the transcript ID and the gene ID.
That mapping is used in the **refine_bio_compendia_etl** task to roll up the gene expressions.

.. table::
    :widths: auto

    ===================    =========  =======
    Data Source            Type       Purpose
    ===================    =========  =======
    TranscriptToGeneMap    Auxiliary  Provides mappings between transcript IDs and gene IDs
    ===================    =========  =======

.. _biocyc_matrix:

-------------
biocyc_matrix
-------------

Turns the BioCyc data into a matrix. The data is aggregated by pathway and any two genes that share a common pathway
are considered a positive pair. The result is a matrix of the form

.. math::
    \mathbf{M}_{ij} = \begin{cases} 1 & \text{two genes share a common pathway} \\ 0 & \text{two genes do not share a common pathway}\\ \end{cases}


.. _interactor_matrix:

-------------------------------------------------------
biogrid_matrix, intact_matrix, mint_matrix, mips_matrix
-------------------------------------------------------

**bins**: ``[0.5, 1.5, 2.5, 4.5]``

Each of these tasks turns its respective interactor dataset into a matrix. The collected data is aggregated over the
gene pairs such that the edge weight between any two genes is the number of distinct interaction methods used to
connect them. The edge weights are then quantized into bins relative to the specified edges above, yielding a
matrix of the form

.. math::
    \mathbf{M}_{ij} = d \in \{0, 1, 2, 3\}

.. _gsea_matrix:

-----------
gsea_matrix
-----------

**bins**: ``[-1.5, -0.5, 0.5, 1.5, 2.5, 3.5, 4.5]``

Transforms the GSEA data into a matrix form. The data is transformed into two matrices indexed by gene and experiment,
with the specific contribution in each cell scaled by the total number of genes within the experiment. In other words,
for the :math:`i^{th}` gene and the :math:`j^{th}` experiment, we obtain a matrix

.. math::
    \mathbf{D}_{ij} = \frac{1}{\sqrt{\Vert \vec{g}_{j} \Vert}}

where :math:`\vec{g}_{j}` is the vector of genes in the :math:`j^{th}` experiment. Then the final matrix is obtained by

.. math::
    \mathbf{B} = \mathbf{D}\mathbf{D}^{T}

and quantizing according to the bins above.


.. _propagate_annotations:

---------------------
propagate_annotations
---------------------

The annotation data obtained from the Gene Ontology specifies which GO terms are to be annotated with which proteins.
Thus, for every term :math:`T_i` there exists some set of proteins :math:`P_i = \{p_1, p_2, \dots, p_n \}` that are
annotated to that term. Annotations are propagated from the leaf terms (most specific) to their parents (most general)
according to the following rules:

1. If the relation connecting the child to the parent is either *PART_OF* or *IS_A*, the all of the child's annotations
   are propagated to the parent. In other words, if the child :math:`T_c` has a set of genes :math:`G_c` and parent
   :math:`T_p` has a set of genes :math:`G_p`, then after propagation the parent will have a set of genes given by
   :math:`G_c \cup G_p`.

2. If the relation connecting the child to the parent is a regulatory relation (one of *REGULATES*, *POSITIVELY_REGULATES*,
   or *NEGATIVELY_REGULATES*), then the child's annotations are propagated to a property of the parent called
   "terminal_annotations", which differ from the constitutitve relations in that they are not propagated further upward.
   In other words, if a term :math:`T_1` propagates its annotations to the terminal annotations of its parent, :math:`T_2`,
   then :math:`T_2` *does not* propagate those same annotations to *its* parent, :math:`T_3`. Annotations which are
   *not* terminal will still be propagated as in the first case. At the end of the propagation procedure,
   terminal annotations are merged with the other annotations for a given node.

.. _map_genes:

---------
map_genes
---------

Once all protein annotations have been properly propagated, the proteins are mapped to genes using the mappings obtained
from the UniProt database. For each term in a set of slims, we produce a list of genes associated with that term. That
list is written to a file, one file for each term with one gene per line of a file. This format is a consequence of
attempts to maintain compatibility with an older codebase and should probably be changed in the future to something
like a pickled dictionary.

.. _build_gold_standard_matrix:

--------------------------
build_gold_standard_matrix
--------------------------

The gold standard matrix represents the known associations between genes. It is built out of both the BioCyc matrix
data and the data produced in the **map_genes** step. The individual context files obtained from the **map_genes**
task are converted into a matrix with the form:

.. math::
    \mathbf{G}_{ij} = \begin{cases} 1 & \text{two genes are coannotated to a positive context}\\
    -1 & \text{two genes are coannotated to a negative context}\\
    0 & \text{otherwise} \end{cases}

This matrix is then combined with the BioCyc matrix in the following way:

.. math::
    \mathbf{M}_{ij} = \begin{cases} 1 & \text{if either } \mathbf{G}_{ij} \text{ or } \mathbf{B}_{ij} \text{ is 1}\\
    -1 & \text{if either } \mathbf{G}_{ij} \text{ or } \mathbf{B}_{ij} \text{ is -1 except as covered by the preceding case}\\
    0 & \text{otherwise} \end{cases}

where :math:`\mathbf{B}` is the matrix obtained in the **biocyc_matrix** task.


.. _build_context_matrices:

----------------------
build_context_matrices
----------------------

In order to compute the counts, and therefore the conditional probability tables that will tell us what each dataset's
contribution is to a given context, we will need to compute the network matrix for each individual context separately.
This procedure is identical to the one described in **build_gold_standard_matrix** with the difference that instead
of combining all of the matrices together, we leave them as separate files. Thus, for every context :math:`C_n` we
obtain a matrix of the form

.. math::
    \mathbf{C}_{ij} = \begin{cases} 1 & \text{these two genes are coannotated to this context}\\
    0 & \text{otherwise} \end{cases}


.. _compute_distance_matrices:

-------------------------
compute_distance_matrices
-------------------------

**bins**: ``[-1.5, -0.5, 0.5, 1.5, 2.5, 3.5, 4.5]``

This task turns the PCL files built by the transformer inside the **refine_bio_etl** task into matrices. Consider a
PCL file which contains genes in the rows and samples in the columns, such that for row *r* and column *c*, we have a
matrix

.. math::
    \mathbf{P}_{rc} = \text{expression level of gene $r$ in sample $c$}

We then compute the Pearson correlation of the row-vectors of :math:`\mathbf{P}`, which is then Fischer-transformed
and z-scaled, i.e.

.. math::
    \begin{eqnarray}
    \mathbf{C} = Pearson(\mathbf{P}) \\
    \mathbf{F} = \tanh^{-1}(\mathbf{C}) \\
    \mathbf{Z} = (\mathbf{F} - \bar{\mathbf{F}}) / \sigma
    \end{eqnarray}

where :math:`\sigma` is the standard deviation of :math:`\mathbf{F}` considered as a single set of numbers, and the
Fischer transform in the second line should be understood to operate on a per-element basis.

This is a long-running task that is farmed out to the cluster. It can take as long as a day to complete if running the
pipeline *de novo*, although partial updates should be relatively fast. For more details on setting up the cluster,
consult the relevant section of this guide.

.. _compute_context_masks:

---------------------
compute_context_masks
---------------------

In order to be able to efficiently compute the bin counts for each positive context, we first need to mask the context
matrices themselves with the gold standard. This is an operation that can be factored out of the counts computation
so that it can be done linearly (in the number of contexts) rather than quadratically (number of contexts times number
of datasets).

The mask is computed by first finding the "bridge genes" which are genes that are present in the gold standard but not
in the context matrix. Then, any gene pair containing a bridge gene and a context gene is masked as ``True`` while
every other pair is masked as ``False``. The positive and negative counts are then computed from the positive and
negative parts of the gold standard matrix as applied to the aforementioned mask. The resultant counts represent the
priors for gene association within the given context.

.. _compute_data_positives:

----------------------
compute_data_positives
----------------------

The data positive of a given dataset matrix is simply the part of the dataset matrix masked by the part of the
gold standard matrix where the corresponding gene pairs are positively associated. In other words, for a given data
matrix :math:`\mathbf{D}`, the resultant data positive matrix is given by

.. math::
    \mathbf{P}_{ij} = \begin{cases} 1 & \text{if } \mathbf{G}_{ij} = 1\\ 0 & \text{otherwise} \end{cases}

where :math:`\mathbf{G}` is the gold standard matrix.

This operation is factored out of the counts computation step because it is common to each context/dataset pair,
which means that it can be done in linear time here rather than in quadratic time during the counts computation, just
like the **compute_counts_mask** step.

.. _compute_counts:

--------------
compute_counts
--------------

This is the core task that actually computes the counts that are used in the Bayesian integration algorithm for
inferring gene function. For each set of context :math:`\mathbf{C}_n`, associated context mask
:math:`\mathbf{M}_n` (from in **compute_context_masks**), data matrix :math:`\mathbf{D}_m`, and associated data
positive :math:`\mathbf{P}_m` (from **compute_data_positives**), we can compute the positive and negative evidence
counts matrices :math:`\mathbf{E}^{+}` and :math:`\mathbf{E}^{-}` by:

.. math::

    \mathbf{E}^{+}_{ij} = \begin{cases}
    \mathbf{P}_{ij} & \text{if } \mathbf{C}_{ij} = 1\\
    -1 & \text{otherwise}\\
    \end{cases}

and

.. math::

    \mathbf{E}^{-}_{ij} = \begin{cases}
    \mathbf{D}_{ij} & \text{if } \mathbf{M}_{ij} = 0\\
    -1 & \text{otherwise}\\
    \end{cases}

The positive and negative evidence counts are then obtained by summing up the bin counts in the respective evidence
matrices.

This task is computationally intensive as the number of operations is equal to the product of the number of contexts
and the number of datasets. As an order of magnitude there are about 500 contexts and 10,000 datasets, for a total of
about :math:`5 \times 10^{6}` operations. These operations are distributes across the cluster and assembled into a
single dictionary structure within the task.

.. _compute_mutual_information:

--------------------------
compute_mutual_information
--------------------------

Before we compute the conditional probabilities, we need to know which datasets contain the most useful signal.
To do this, we compute the mutual information of every dataset pair via the standard mutual information formula:

.. math::
    w_{ij} = \frac{1}{L_p} \sum \ln \frac{H(\mathbf{D}_i, \mathbf{D}_j)}{H(\mathbf{D}_i) \otimes H(\mathbf{D}_j)}

where :math:`L_p` is the Laplace scaling factor, :math:`H(\mathbf{D}_i, \mathbf{D}_j)` is the joint histogram of the
data matrices, and :math:`H(\mathbf{D}_i)` is the histogram of one matrix. The denominator in the above expression
is a cross product term, and this the whole division and log is on a per-element basis. The sum is over all the
elements of the matrix.

.. warning::

    This is an extremely expensive computation, which, like all expensive computations, are distributed onto the
    cluster. However, because of the quadratic nature of this problem, were we to recompute this afresh every time,
    we would spend many days computing :math:`10^8` dataset pairs. We can take advantage of the fact that dataset
    matrices will never change, and only do this computation once; intermediate products of the computation are then
    stored in a large matrix with the dataset name on both axes. The task will check for the presence of a non-NaN
    entry for a given matrix pair and only compute the mutual information for that pair if it is missing. This means
    that every time beyond the first that this task runs, you will only be computing on the new datasets you have
    obtained. Of course, this operation is still going to be very expensive because any additional datasets still
    have to be paired with all of the old ones.

    In order to increase the efficiency of the computation, the histogram functions are written as Cython extensions.
    They are discussed in their own section of this manual.


.. _calculate_alphas:

----------------
calculate_alphas
----------------

Using the mutual information matrix obtained above, we can write, for a dataset *i*:

.. math::
    \alpha_i = \frac{1}{p} 2^{\frac{\sum_{j} w_{ij} - w_{jj}}{w_{jj}}} - 1

where :math:`p` is the pseudocounts.

.. _compute_conditional_probabilities:

---------------------------------
compute_conditional_probabilities
---------------------------------

With the alphas information, we can now translate the counts obtained in **compute_counts** into conditional
probabilities. For each dataset *k*, set of counts :math:`b_i` indexed by the bin label :math:`i \in {0,\dots, n}`,
where *n* is the number of bins (usually 8), given pseudocounts :math:`p`, and total counts :math:`T = \sum_i^n b_i`,
we can compute the probability in the following way:

.. math::
    p_i = \begin{cases} 1/n & \text{if } T = 0 \text{ or } \alpha_k = 0 \\
    \frac{s \times (b_i + 1) + \alpha_k}{T + (\alpha_k \times n)} & \text{otherwise}\\
    \end{cases}

where :math:`s = p / T` is the scale factor, which is 1 if :math:`p` is 0.

.. _align_with_gene_list:

--------------------
align_with_gene_list
--------------------

We are only interested in genes that either code for proteins or rRNA. For efficiency reasons we also want to align
all of the data matrices to the same gene set. This task selects out the relevant set of genes and then reindexes the
dataset matrices to those genes, using -1 as the fill value for missing nodes.


.. warning::
    If you end up doing anything that changes the list of relevant genes (which mostly means rerunning the **_etl**
    tasks that are responsible for importing and mapping new genes), you *must* rerun this task to make sure that the
    data matrices are aligned appropriately. Without this, downstream tasks will fail when looking for genes that end
    up missing from the data.

.. _compute_aligned_posteriors:

--------------------------
compute_aligned_posteriors
--------------------------

This task is the core of the Bayesian integration procedure. For each context :math:`C_n` and dataset :math:`D_m`,
aligned as described above, and given the conditional probability table :math:`CPT_{nm}` for each context/dataset pair,
we can compute the resultant posterior context matrices corresponding to the positive and negative evidence as follows:

.. math::
    \begin{eqnarray}
    \mathbf{P}^{n}_{ij} = \sum_m \ln (\mathbf{CPT}^{+}_{nm}[\mathbf{D}^{m}_{ij}]) \\
    \mathbf{N}^{n}_{ij} = \sum_m \ln (\mathbf{CPT}^{-}_{nm}[\mathbf{D}^{m}_{ij}]) \\
    \end{eqnarray}

Here the superscripts should be understood as indexing operations, not exponents, and the square brackets are an
"array access" operation, so that for any value of bin :math:`b = \mathbf{D}^{m}_{ij}`, the values
:math:`\mathbf{CPT}^{+}_{nm}[b]` and :math:`\mathbf{CPT}^{-}_{nm}[b]` represent the positive and negative evidence
respectively obtaind from the conditional probability table for the given context/dataset pair.

The final evidence matrix is then given by

.. math::
    \mathbf{E}_n = \frac{1}{1 + e^{\mathbf{N}_n - \mathbf{P}_n}}

where the exponentiation and division is understood elementwise.

.. warning::
    The math of this task is fairly trivial, but the actual distributed computation is extremely complex, for good
    reason. We would like to parallelize operations over the contexts in the outer loop and then over the datasets
    in the inner loop, and then distribute that work across a cluster. This is done by having one Celery worker per
    cluster node be responsible for the context; that worker then distributes the work of actually computing the
    individual pieces of evidence to sub-processes spawned from within the worker. More details are available in the
    documentation for the individual functions themselves.


.. _average_context_networks:

------------------------
average_context_networks
------------------------

In this step we simply perform an arithmetic average over the :math:`E_n` matrices obtained in the integration step.
The result is a single network averaged over all the contexts.

.. _compute_gene_sets:

-----------------
compute_gene_sets
-----------------

Analogously to the **map_genes** task, we want to compute the gene sets for the positive and negative slim terms,
and also for all of the terms together. This is done in a separate task to factor out common work.

.. _learn_svm_weights:

-----------------
learn_svm_weights
-----------------

Given the gene sets as above, for a given term :math:`T_i`, we can now compute a set of positive and negative examples
for the SVM to learn on. If :math:`G` is the set of all genes annotated to any GO term in the *biological_process*
branch of the ontology and :math:`S` is the set of genes annotated to the negative slim that is the ancestor of
:math:`T_i`, then the positive examples :math:`P` are just given by the terms annotated to :math:`T_i` and the negative
examples are given by :math:`G - S`. These examples are then used to train the SVM on the averaged context network
from above, and used to learn the scores of all the genes in the testing set. Within each context, a stratified K-fold
is used and the scores are averaged over. The result is a set of tables in CSV format, one per GO term, which contain
the gene and its SVM score.

.. _svm_to_probabilities:

--------------------
svm_to_probabilities
--------------------

The SVM scores are converted to probabilities via Platt scaling. The output is a single file containing on each row
the GO term, the gene Entrez ID, and the scaled probability of the gene being expressed in the given process. This file
is suitable for importing directly into the HumanBase web backend.